<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Production Considerations &mdash; Fuel for OpenStack 3.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '3.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Fuel for OpenStack 3.0 documentation" href="../index.html" />
    <link rel="next" title="FAQ (Frequently Asked Questions)" href="0060-frequently-asked-questions.html" />
    <link rel="prev" title="Create a multi-node OpenStack cluster using Fuel" href="0050-installation-instructions.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="0060-frequently-asked-questions.html" title="FAQ (Frequently Asked Questions)"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="0050-installation-instructions.html" title="Create a multi-node OpenStack cluster using Fuel"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Fuel for OpenStack 3.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Production Considerations</a><ul>
<li><a class="reference internal" href="#sizing-hardware">Sizing Hardware</a><ul>
<li><a class="reference internal" href="#processing">Processing</a></li>
<li><a class="reference internal" href="#memory">Memory</a></li>
<li><a class="reference internal" href="#storage-space">Storage Space</a><ul>
<li><a class="reference internal" href="#throughput">Throughput</a></li>
<li><a class="reference internal" href="#remote-storage">Remote storage</a></li>
<li><a class="reference internal" href="#object-storage">Object storage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#networking">Networking</a><ul>
<li><a class="reference internal" href="#scalability-and-oversubscription">Scalability and oversubscription</a></li>
<li><a class="reference internal" href="#hardware-for-this-example">Hardware for this example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#redeploying-an-environment">Redeploying An Environment</a><ul>
<li><a class="reference internal" href="#environments">Environments</a></li>
<li><a class="reference internal" href="#deployment-pipeline">Deployment pipeline</a></li>
<li><a class="reference internal" href="#links">Links</a></li>
</ul>
</li>
<li><a class="reference internal" href="#large-scale-deployments">Large Scale Deployments</a><ul>
<li><a class="reference internal" href="#certificate-signing-requests-and-puppet-master-cobbler-capacity">Certificate signing requests and Puppet Master/Cobbler capacity</a></li>
<li><a class="reference internal" href="#downloading-of-operating-systems-and-other-software">Downloading of operating systems and other software</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="0050-installation-instructions.html"
                        title="previous chapter">Create a multi-node OpenStack cluster using Fuel</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="0060-frequently-asked-questions.html"
                        title="next chapter">FAQ (Frequently Asked Questions)</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/pages/0055-production-considerations.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="production-considerations">
<span id="production"></span><h1>Production Considerations<a class="headerlink" href="#production-considerations" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#sizing-hardware" id="id2">Sizing Hardware</a><ul>
<li><a class="reference internal" href="#processing" id="id3">Processing</a></li>
<li><a class="reference internal" href="#memory" id="id4">Memory</a></li>
<li><a class="reference internal" href="#storage-space" id="id5">Storage Space</a><ul>
<li><a class="reference internal" href="#throughput" id="id6">Throughput</a></li>
<li><a class="reference internal" href="#remote-storage" id="id7">Remote storage</a></li>
<li><a class="reference internal" href="#object-storage" id="id8">Object storage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#networking" id="id9">Networking</a><ul>
<li><a class="reference internal" href="#scalability-and-oversubscription" id="id10">Scalability and oversubscription</a></li>
<li><a class="reference internal" href="#hardware-for-this-example" id="id11">Hardware for this example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary" id="id12">Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#redeploying-an-environment" id="id13">Redeploying An Environment</a><ul>
<li><a class="reference internal" href="#environments" id="id14">Environments</a></li>
<li><a class="reference internal" href="#deployment-pipeline" id="id15">Deployment pipeline</a></li>
<li><a class="reference internal" href="#links" id="id16">Links</a></li>
</ul>
</li>
<li><a class="reference internal" href="#large-scale-deployments" id="id17">Large Scale Deployments</a><ul>
<li><a class="reference internal" href="#certificate-signing-requests-and-puppet-master-cobbler-capacity" id="id18">Certificate signing requests and Puppet Master/Cobbler capacity</a></li>
<li><a class="reference internal" href="#downloading-of-operating-systems-and-other-software" id="id19">Downloading of operating systems and other software</a></li>
</ul>
</li>
</ul>
</div>
<p>Fuel simplifies the set up of an OpenStack cluster, affording you the ability to dig in and fully understand how OpenStack works. You can deploy on test hardware or in a virtualized environment and root around all you like, but when it comes time to deploy to production there are a few things to take into consideration.</p>
<p>In this section we will talk about such things including how to size your hardware and how to handle large-scale deployments.</p>
<div class="section" id="sizing-hardware">
<h2><a class="toc-backref" href="#id2">Sizing Hardware</a><a class="headerlink" href="#sizing-hardware" title="Permalink to this headline">¶</a></h2>
<p>One of the first questions people ask when planning an OpenStack deployment is &#8220;what kind of hardware do I need?&#8221; There is no such thing as a one-size-fits-all answer, but there are straightforward rules to selecting appropriate hardware that will suit your needs. The Golden Rule, however, is to always accomodate for growth. With the potential for growth accounted for, you can move on to the actual hardware needs.</p>
<p>Many factors contribute to selecting hardware for an OpenStack cluster &#8211; <a class="reference external" href="http://www.mirantis.com/contact/">contact Mirantis</a> for information on your specific requirements &#8211; but in general, you will want to consider the following factors:</p>
<ul class="simple">
<li>Processing</li>
<li>Memory</li>
<li>Storage</li>
<li>Networking</li>
</ul>
<p>Your needs in each of these areas are going to determine your overall hardware requirements.</p>
<div class="section" id="processing">
<h3><a class="toc-backref" href="#id3">Processing</a><a class="headerlink" href="#processing" title="Permalink to this headline">¶</a></h3>
<p>In order to calculate how much processing power you will need to aquire you will need to determine the number of VMs your cloud will support. You must also consider the average and maximum processor resources you will allocate to each VM. In the vast majority of deployments, the allocated resources will be the same for all of your VMs. However, if you are planning to create groups of VMs that have different requirements, you will need to calculate for all of them in aggregate. Consider this example:</p>
<ul class="simple">
<li>100 VMs</li>
<li>2 EC2 compute units (2 GHz) average</li>
<li>16 EC2 compute units (16 GHz) max</li>
</ul>
<p>To make it possible to provide the maximum CPU in this example you will need at least 5 cores (16 GHz/(2.4 GHz per core * 1.3 to adjust for hyperthreading)) per machine, and at least 84 cores ((100 VMs * 2 GHz per VM)/2.4 GHz per core) in total. If you were to select the Intel E5 2650-70 8 core CPU, that means you need 11 sockets (84 cores / 8 cores per socket). This breaks down to six dual core servers (12 sockets / 2 sockets per server), for a &#8220;packing density&#8221; of 17 VMs per server (102 VMs / 6 servers).</p>
<p>This process also accomodates growth since you now know what a single server using this CPU configuration can support. You can add new servers accounting for 17 VMs each as needed without having to re-calculate.</p>
<p>You will also need to take into account the following:</p>
<ul class="simple">
<li>This model assumes you are not oversubscribing your CPU.</li>
<li>If you are considering Hyperthreading, count each core as 1.3, not 2.</li>
<li>Choose a good value CPU that supports the technologies you require.</li>
</ul>
</div>
<div class="section" id="memory">
<h3><a class="toc-backref" href="#id4">Memory</a><a class="headerlink" href="#memory" title="Permalink to this headline">¶</a></h3>
<p>Continuing to use the example from the previous section, we need to determine how much RAM will be required to support 17 VMs per server. Let&#8217;s assume that you need an average of 4GBs of RAM per VM with dynamic allocation for up to 12GBs for each VM. Calculating that all VMs will be using 12GBs of RAM requires that each server have 204GBs of available RAM.</p>
<p>You must also consider that the node itself needs sufficient RAM to accomodate core OS operations as well as RAM for each VM container (not the RAM allocated to each VM, but the memory the core OS uses to run the VM). The node&#8217;s OS must run it&#8217;s own operations, schedule proccesses, allocate dynamic resources, and handle network operations, so giving the node itself at least 16GBs or more RAM is not unreasonable.</p>
<p>Considering that the RAM we would consider for servers comes in 4GB, 8GB, 16GB, and 32GB sticks, we would need a total of 265GBs of RAM installed per server. For an average 2-CPU server board you get 16-24 RAM slots. To have 256GBs installed you would need sixteen 16GB sticks of RAM to satisfy your RAM needs for up to 17 VMs requiring dynamic allocation up to 12GBs and to support all core OS requirements.</p>
<p>You can adjust this calculation based on your needs.</p>
</div>
<div class="section" id="storage-space">
<h3><a class="toc-backref" href="#id5">Storage Space</a><a class="headerlink" href="#storage-space" title="Permalink to this headline">¶</a></h3>
<p>When it comes to disk space there are several types that you need to consider:</p>
<ul class="simple">
<li>Ephemeral (the local drive space for a VM)</li>
<li>Persistent (the remote volumes that can be attached to a VM)</li>
<li>Object Storage (such as images or other objects)</li>
</ul>
<p>As far as local drive space that must reside on the compute nodes, in our example of 100 VMs we make the following assumptions:</p>
<ul class="simple">
<li>150 GB local storage per VM</li>
<li>5 TB total of local storage (100 VMs * 50 GB per VM)</li>
<li>500 GB of persistent volume storage per VM</li>
<li>50 TB total persistent storage</li>
</ul>
<p>Returning to our already established example, we need to figure out how much storage to install per server. This storage will service the 17 VMs per server. If we are assuming 50GBs of storage for each VMs drive container, then we would need to install 2.5TBs of storage on the server. Since most servers have anywhere from 4 to 32 2.5&#8221; drive slots or 2 to 12 3.5&#8221; drive slots, depending on server form factor (i.e., 2U vs. 4U), you will need to consider how the storage will be impacted by the intended use.</p>
<p>If storage impact is not expected to be significant, then you may consider using unified storage. For this example a single 3TB drive would provide more than enough storage for 17 150GB VMs. If speed is really not an issue, you might even consider installing two or three 3TB drives and configure a RAID-1 or RAID-5 for redundancy. If speed is critical, however, you will likely want to have a single hardware drive for each VM. In this case you would likely look at a 3U form factor with 24-slots.</p>
<p>Don&#8217;t forget that you will also need drive space for the node itself, and don&#8217;t forget to order the correct backplane that supports the drive configuration that meets your needs. Using our example specifications and assuming that speed it critical, a single server would need 18 drives, most likely 2.5&#8221; 15,000RPM 146GB SAS drives.</p>
<div class="section" id="throughput">
<h4><a class="toc-backref" href="#id6">Throughput</a><a class="headerlink" href="#throughput" title="Permalink to this headline">¶</a></h4>
<p>As far as throughput, that&#8217;s going to depend on what kind of storage you choose.  In general, you calculate IOPS based on the packing density (drive IOPS * drives in the server / VMs per server), but the actual drive IOPS will depend on the drive technology you choose.  For example:</p>
<ul>
<li><p class="first">3.5&#8221; slow and cheap (100 IOPS per drive, with 2 mirrored drives)</p>
<blockquote>
<div><ul class="simple">
<li>100 IOPS * 2 drives / 17 VMs per server = 12 Read IOPS, 6 Write IOPS</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">2.5&#8221; 15K (200 IOPS, 4 600 GB drive, RAID 10)</p>
<blockquote>
<div><ul class="simple">
<li>200 IOPS * 4 drives / 17 VMs per server = 48 Read IOPS, 24 Write IOPS</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">SSD (40K IOPS, 8 300 GB drive, RAID 10)</p>
<blockquote>
<div><ul class="simple">
<li>40K * 8 drives / 17 VMs per server = 19K Read IOPS, 9.5K Write IOPS</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Clearly, SSD gives you the best performance, but the difference in cost between SSDs and the less costly platter-based solutions is going to be signficant, to say the least. The acceptable cost burden is determined by the balance between your budget and your performance and redundancy needs. It is also important to note that the rules for redundancy in a cloud environment are different than a traditional server installation in that entire servers provide redundancy as opposed to making a single server instance redundant.</p>
<p>In other words, the weight for redundant components shifts from individual OS installation to server redundancy. It is far more critical to have redundant power supplies and hot-swappable CPUs and RAM than to have redundant compute node storage. If, for example, you have 18 drives installed on a server and have 17 drives directly allocated to each VM installed and one fails, you simply replace the drive and push a new node copy. The remaining VMs carry whatever additional load is present due to the temporary loss of one node.</p>
</div>
<div class="section" id="remote-storage">
<h4><a class="toc-backref" href="#id7">Remote storage</a><a class="headerlink" href="#remote-storage" title="Permalink to this headline">¶</a></h4>
<p>IOPS will also be a factor in determining how you plan to handle persistent storage.  For example, consider these options for laying out your 50 TB of remote volume space:</p>
<ul class="simple">
<li>12 drive storage frame using 3 TB 3.5&#8221; drives mirrored<ul>
<li>36 TB raw, or 18 TB usable space per 2U frame</li>
<li>3 frames (50 TB / 18 TB per server)</li>
<li>12 slots x 100 IOPS per drive = 1200 Read IOPS, 600 Write IOPS per frame</li>
<li>3 frames x 1200 IOPS per frame / 100 VMs = 36 Read IOPS, 18 Write IOPS per VM</li>
</ul>
</li>
<li>24 drive storage frame using 1TB 7200 RPM 2.5&#8221; drives<ul>
<li>24 TB raw, or 12 TB usable space per 2U frame</li>
<li>5 frames (50 TB / 12 TB per server)</li>
<li>24 slots x 100 IOPS per drive = 2400 Read IOPS, 1200 Write IOPS per frame</li>
<li>5 frames x 2400 IOPS per frame / 100 VMs = 120 Read IOPS, 60 Write IOPS per frame</li>
</ul>
</li>
</ul>
<p>You can accomplish the same thing with a single 36 drive frame using 3 TB drives, but this becomes a single point of failure in your cluster.</p>
</div>
<div class="section" id="object-storage">
<h4><a class="toc-backref" href="#id8">Object storage</a><a class="headerlink" href="#object-storage" title="Permalink to this headline">¶</a></h4>
<p>When it comes to object storage, you will find that you need more space than you think.  For example, this example specifies 50 TB of object storage.  Easy right?  Not really.  Object storage uses a default of 3 times the required space for replication, which means you will need 150 TB.  However, to accommodate two hands-off zones, you will need 5 times the required space, which actually means 250 TB.  The calculations don&#8217;t end there.  You don&#8217;t ever want to run out of space, so &#8220;full&#8221; should really be more like 75% of capacity, which means you will need a total of 333 TB, or a multiplication factor of 6.66.</p>
<p>Of course, that might be a bit much to start with; you might want to start with a happy medium of a multiplier of 4, then acquire more hardware as your drives begin to fill up.  That calculates to 200 TB in our example.  So how do you put that together?  If you were to use 3 TB 3.5&#8221; drives, you could use a 12 drive storage frame, with 6 servers hosting 36 TB each (for a total of 216 TB).  You could also use a 36 drive storage frame, with just 2 servers hosting 108 TB each, but its not recommended due to the high cost of failure to replication and capacity issues.</p>
</div>
</div>
<div class="section" id="networking">
<h3><a class="toc-backref" href="#id9">Networking</a><a class="headerlink" href="#networking" title="Permalink to this headline">¶</a></h3>
<p>Perhaps the most complex part of designing an OpenStack cluster is the networking.  An OpenStack cluster can involve multiple networks even beyond the Public, Private, and Internal networks.  Your cluster may involve tenant networks, storage networks, multiple tenant private networks, and so on.  Many of these will be VLANs, and all of them will need to be planned out in advance to avoid configuration issues.</p>
<p>In terms of the example network, consider these assumptions:</p>
<ul class="simple">
<li>100 Mbits/second per VM</li>
<li>HA architecture</li>
<li>Network Storage is not latency sensitive</li>
</ul>
<p>In order to achieve this, you can use two 1Gb links per server (2 x 1000 Mbits/second / 17 VMs = 118 Mbits/second).  Using two links also helps with HA.  You can also increase throughput and decrease latency by using 2 10 Gb links, bringing the bandwidth per VM to 1 Gb/second, but if you&#8217;re going to do that, you&#8217;ve got one more factor to consider.</p>
<div class="section" id="scalability-and-oversubscription">
<h4><a class="toc-backref" href="#id10">Scalability and oversubscription</a><a class="headerlink" href="#scalability-and-oversubscription" title="Permalink to this headline">¶</a></h4>
<p>It is one of the ironies of networking that 1Gb Ethernet generally scales better than 10Gb Ethernet &#8211; at least until 100Gb switches are more commonly available.  It&#8217;s possible to aggregate the 1Gb links in a 48 port switch, so that you have 48 1Gb links down, but 4 10GB links up.  Do the same thing with a 10Gb switch, however, and you have 48 10Gb links down and 4 100Gb links up, resulting in oversubscription.</p>
<p>Like many other issues in OpenStack, you can avoid this problem to a great extent with careful planning.  Problems only arise when you are moving between racks, so plan to create &#8220;pods&#8221;, each of which includes both storage and compute nodes.  Generally, a pod is the size of a non-oversubscribed L2 domain.</p>
</div>
<div class="section" id="hardware-for-this-example">
<h4><a class="toc-backref" href="#id11">Hardware for this example</a><a class="headerlink" href="#hardware-for-this-example" title="Permalink to this headline">¶</a></h4>
<p>In this example, you are looking at:</p>
<ul class="simple">
<li>2 data switches (for HA), each with a minimum of 12 ports for data (2 x 1Gb links per server x 6 servers)</li>
<li>1 1Gb switch for IPMI (1 port per server x 6 servers)</li>
<li>Optional Cluster Management switch, plus a second for HA</li>
</ul>
<p>Because your network will in all likelihood grow, it&#8217;s best to choose 48 port switches.  Also, as your network grows, you will need to consider uplinks and aggregation switches.</p>
</div>
</div>
<div class="section" id="summary">
<h3><a class="toc-backref" href="#id12">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>In general, your best bet is to choose a 2 socket server with a balance in I/O, CPU, Memory, and Disk that meets your project requirements.  Look for a 1U R-class or 2U high density C-class server.  Some good options from Dell for compute nodes include:</p>
<ul class="simple">
<li>Dell PowerEdge R620</li>
<li>Dell PowerEdge C6220 Rack Server</li>
<li>Dell PowerEdge R720XD (for high disk or IOPS requirements)</li>
</ul>
<p>You may also want to consider systems from HP (<a class="reference external" href="http://www.hp.com/servers">http://www.hp.com/servers</a>) or from a smaller systems builder like Aberdeen, a manufacturer that specializes in powerful, low-cost systems and storage servers (<a class="reference external" href="http://www.aberdeeninc.com">http://www.aberdeeninc.com</a>).</p>
</div>
</div>
<div class="section" id="redeploying-an-environment">
<h2><a class="toc-backref" href="#id13">Redeploying An Environment</a><a class="headerlink" href="#redeploying-an-environment" title="Permalink to this headline">¶</a></h2>
<p>Because Puppet is additive only, there is no ability to revert changes as you would in a typical application deployment. If a change needs to be backed out, you must explicitly add a configuration to reverse it, check the configuration in, and promote it to production using the pipeline. This means that if a breaking change does get deployed into production, typically a manual fix is applied, with the proper fix subsequently checked into version control.</p>
<p>Fuel offers the ability to isolate code changes while developing a deployment and minimizes the headaches associated with maintaining multiple configurations through a single Puppet Master by creating what are called environments.</p>
<div class="section" id="environments">
<h3><a class="toc-backref" href="#id14">Environments</a><a class="headerlink" href="#environments" title="Permalink to this headline">¶</a></h3>
<p>Puppet supports assigning nodes &#8216;environments&#8217;. These environments can be mapped directly to your development, QA and production life cycles, so it’s a way to distribute code to nodes that are assigned to those environments.</p>
<ul>
<li><p class="first">On the Master/Server Node</p>
<p>The Puppet Master tries to find modules using its <tt class="docutils literal"><span class="pre">modulepath</span></tt> setting, which by default is <tt class="docutils literal"><span class="pre">/etc/puppet/modules</span></tt>. It is common practice to set this value once in your <tt class="docutils literal"><span class="pre">/etc/puppet/puppet.conf</span></tt>.  Environments expand on this idea and give you the ability to use different settings for different configurations.</p>
<p>For example, you can specify several search paths. The following example dynamically sets the <tt class="docutils literal"><span class="pre">modulepath</span></tt> so Puppet will check a per-environment folder for a module before serving it from the main set:</p>
<div class="highlight-python"><pre>[master]
  modulepath = $confdir/$environment/modules:$confdir/modules

[production]
  manifest   = $confdir/manifests/site.pp

[development]
  manifest   = $confdir/$environment/manifests/site.pp</pre>
</div>
</li>
<li><p class="first">On the Agent Node</p>
<p>Once the agent node makes a request, the Puppet Master gets informed of its environment. If you don’t specify an environment, the agent uses the default <tt class="docutils literal"><span class="pre">production</span></tt> environment.</p>
<p>To set an environment agent-side, just specify the environment setting in the <tt class="docutils literal"><span class="pre">[agent]</span></tt> block of <tt class="docutils literal"><span class="pre">puppet.conf</span></tt>:</p>
<div class="highlight-python"><pre>[agent]
  environment = development</pre>
</div>
</li>
</ul>
</div>
<div class="section" id="deployment-pipeline">
<h3><a class="toc-backref" href="#id15">Deployment pipeline</a><a class="headerlink" href="#deployment-pipeline" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first">Deploy</p>
<p>In order to deploy multiple environments that don&#8217;t interfere with each other, you should specify the <tt class="docutils literal"><span class="pre">$deployment_id</span></tt> option in <tt class="docutils literal"><span class="pre">/etc/puppet/manifests/site.pp</span></tt>.  It should be an even integer value in the range of 2-254.</p>
<p>This value is used in dynamic environment-based tag generation.  Fuel applies that tag globally to all resources on each node.  It is also used for the keepalived daemon, which evaluates a unique <tt class="docutils literal"><span class="pre">virtual_router_id</span></tt>.</p>
</li>
<li><p class="first">Clean/Revert</p>
<p>At this stage you just need to make sure the environment has the original/virgin state.</p>
</li>
<li><p class="first">Puppet node deactivate</p>
<p>This will ensure that any resources exported by that node will stop appearing in the catalogs served to the agent nodes:</p>
<div class="highlight-python"><pre>puppet node deactivate &lt;node&gt;</pre>
</div>
<p>where <tt class="docutils literal"><span class="pre">&lt;node&gt;</span></tt> is the fully qualified domain name as seen in <tt class="docutils literal"><span class="pre">puppet</span> <span class="pre">cert</span> <span class="pre">list</span> <span class="pre">--all</span></tt>.</p>
<p>You can deactivate nodes manually one by one, or execute the following command to automatically deactivate all nodes:</p>
<div class="highlight-python"><pre>cert list --all | awk '! /DNS:puppet/ { gsub(/"/, "", $2); print $2}' | xargs puppet node deactivate</pre>
</div>
</li>
<li><p class="first">Redeploy</p>
<p>Start the puppet agent again to apply a desired node configuration.</p>
</li>
</ul>
</div>
<div class="section" id="links">
<h3><a class="toc-backref" href="#id16">Links</a><a class="headerlink" href="#links" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://puppetlabs.com/blog/a-deployment-pipeline-for-infrastructure/">http://puppetlabs.com/blog/a-deployment-pipeline-for-infrastructure/</a></li>
<li><a class="reference external" href="http://docs.puppetlabs.com/guides/environment.html">http://docs.puppetlabs.com/guides/environment.html</a></li>
</ul>
</div>
</div>
<div class="section" id="large-scale-deployments">
<h2><a class="toc-backref" href="#id17">Large Scale Deployments</a><a class="headerlink" href="#large-scale-deployments" title="Permalink to this headline">¶</a></h2>
<p>When deploying large clusters &#8211; those of 100 nodes or more &#8211; there are two basic bottlenecks:</p>
<ul class="simple">
<li>Certificate signing requests and Puppet Master/Cobbler capacity</li>
<li>Downloading of operating systems and other software</li>
</ul>
<p>Careful planning is key to eliminating these potential problem areas, but there&#8217;s another way. If you are deploying Fuel 3.1 from the ISO, Fuel takes care of these problems through caching and orchestration. We feel, however, that it&#8217;s always good to have a sense of how to solve these problems should they appear.</p>
<div class="section" id="certificate-signing-requests-and-puppet-master-cobbler-capacity">
<h3><a class="toc-backref" href="#id18">Certificate signing requests and Puppet Master/Cobbler capacity</a><a class="headerlink" href="#certificate-signing-requests-and-puppet-master-cobbler-capacity" title="Permalink to this headline">¶</a></h3>
<p>When deploying a large cluster, you may find that Puppet Master begins to have difficulty when you start exceeding 20 or more simultaneous requests. Part of this problem is because the initial process of requesting and signing certificates involves *.tmp files that can create conflicts.  To solve this problem, you have two options: reduce the number of simultaneous requests, or increase the number of Puppet Master/Cobbler servers.</p>
<p>The number of simultaneous certificate requests that are active can be controlled by staggering the Puppet agent run schedule.  This can be accomplished through orchestration.  You don&#8217;t need extreme staggering &#8211; 1 to 5 seconds will do &#8211; but if this method isn&#8217;t practical, you can increase the number of Puppet Master/Cobbler servers.</p>
<p>If you&#8217;re simply overwhelming the Puppet Master process and not running into file conflicts, one way to get around this problem is to use Puppet Master with Thin as the backend component and nginx as a front end component.  This configuration dynamically scales the number of Puppet Master processes to better accommodate changing load.</p>
<p>You can find sample configuration files for nginx and puppetmasterd at [CONTENT NEEDED HERE].</p>
<p>You can also increase the number of servers by creating a cluster that utilizes a round robin DNS configuration through a service like HAProxy. You will need to ensure that these nodes are kept in sync.  For Cobbler, that means a combination of the &#8211;replicate switch, XMLRPC for metadata, rsync for profiles and distributions.  Similarly, Puppet Master and PuppetDB can be kept in sync with a combination of rsync (for modules, manifests, and SSL data) and database replication.</p>
<img alt="../_images/cobbler-puppet-ha.png" src="../_images/cobbler-puppet-ha.png" />
</div>
<div class="section" id="downloading-of-operating-systems-and-other-software">
<h3><a class="toc-backref" href="#id19">Downloading of operating systems and other software</a><a class="headerlink" href="#downloading-of-operating-systems-and-other-software" title="Permalink to this headline">¶</a></h3>
<p>Large deployments can also suffer from a bottleneck in terms of the additional traffic created by downloading software from external sources.  One way to avoid this problem is by increasing LAN bandwidth through bonding multiple gigabit interfaces.  You might also want to consider 10G Ethernet trunking between infrastructure switches using CAT-6a or fiber to improve backend speeds to reduce latency and provide more overall pipe.  (See &#8220;Sizing Hardware&#8221; for more information on choosing networking equipment.)</p>
<p>Another option is to prevent the need to download so much data in the first place using either apt-cacher to cache frequently downloaded packages or to set up a private repository. The downside of using your own repository, however, is that you have to spend more time manually updating it. Apt-cacher automates this process. To use apt-cacher, the kickstart that Cobbler sends to each node should specify Cobbler&#8217;s IP address and the apt-cacher port as the proxy server.  This will prevent all of the nodes from having to download the software individually.</p>
<p><a class="reference external" href="http://www.mirantis.com/contact/">Contact Mirantis</a> for information on creating a private repository.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="0060-frequently-asked-questions.html" title="FAQ (Frequently Asked Questions)"
             >next</a> |</li>
        <li class="right" >
          <a href="0050-installation-instructions.html" title="Create a multi-node OpenStack cluster using Fuel"
             >previous</a> |</li>
        <li><a href="../index.html">Fuel for OpenStack 3.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Mirantis.
      Last updated on 2013/07/22.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2b1.
    </div>
  </body>
</html>