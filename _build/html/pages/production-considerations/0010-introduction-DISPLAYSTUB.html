<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Sizing Hardware &mdash; Fuel for OpenStack 3.0 documentation</title>
    
    <link rel="stylesheet" href="../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '3.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="top" title="Fuel for OpenStack 3.0 documentation" href="../../index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li><a href="../../index.html">Fuel for OpenStack 3.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Sizing Hardware</a><ul>
<li><a class="reference internal" href="#processing">Processing</a></li>
<li><a class="reference internal" href="#memory">Memory</a></li>
<li><a class="reference internal" href="#storage-space">Storage Space</a><ul>
<li><a class="reference internal" href="#throughput">Throughput</a></li>
<li><a class="reference internal" href="#remote-storage">Remote storage</a></li>
<li><a class="reference internal" href="#object-storage">Object storage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#networking">Networking</a><ul>
<li><a class="reference internal" href="#scalability-and-oversubscription">Scalability and oversubscription</a></li>
<li><a class="reference internal" href="#hardware-for-this-example">Hardware for this example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../_sources/pages/production-considerations/0010-introduction-DISPLAYSTUB.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <p>Fuel simplifies the set up of an OpenStack cluster, affording you the ability to dig in and fully understand how OpenStack works. You can deploy on test hardware or in a virtualized environment and root around all you like, but when it comes time to deploy to production there are a few things to take into consideration.</p>
<p>In this section we will talk about such things including how to size your hardware and how to handle large-scale deployments.</p>
<div class="section" id="sizing-hardware">
<h1>Sizing Hardware<a class="headerlink" href="#sizing-hardware" title="Permalink to this headline">¶</a></h1>
<p>One of the first questions people ask when planning an OpenStack deployment is &#8220;what kind of hardware do I need?&#8221; There is no such thing as a one-size-fits-all answer, but there are straightforward rules to selecting appropriate hardware that will suit your needs. The Golden Rule, however, is to always accomodate for growth. With the potential for growth accounted for, you can move on to the actual hardware needs.</p>
<p>Many factors contribute to selecting hardware for an OpenStack cluster &#8211; <a class="reference external" href="http://www.mirantis.com/contact/">contact Mirantis</a> for information on your specific requirements &#8211; but in general, you will want to consider the following factors:</p>
<ul class="simple">
<li>Processing</li>
<li>Memory</li>
<li>Storage</li>
<li>Networking</li>
</ul>
<p>Your needs in each of these areas are going to determine your overall hardware requirements.</p>
<div class="section" id="processing">
<h2>Processing<a class="headerlink" href="#processing" title="Permalink to this headline">¶</a></h2>
<p>In order to calculate how much processing power you will need to aquire you will need to determine the number of VMs your cloud will support. You must also consider the average and maximum processor resources you will allocate to each VM. In the vast majority of deployments, the allocated resources will be the same for all of your VMs. However, if you are planning to create groups of VMs that have different requirements, you will need to calculate for all of them in aggregate. Consider this example:</p>
<ul class="simple">
<li>100 VMs</li>
<li>2 EC2 compute units (2 GHz) average</li>
<li>16 EC2 compute units (16 GHz) max</li>
</ul>
<p>To make it possible to provide the maximum CPU in this example you will need at least 5 cores (16 GHz/(2.4 GHz per core * 1.3 to adjust for hyperthreading)) per machine, and at least 84 cores ((100 VMs * 2 GHz per VM)/2.4 GHz per core) in total. If you were to select the Intel E5 2650-70 8 core CPU, that means you need 11 sockets (84 cores / 8 cores per socket). This breaks down to six dual core servers (12 sockets / 2 sockets per server), for a &#8220;packing density&#8221; of 17 VMs per server (102 VMs / 6 servers).</p>
<p>This process also accomodates growth since you now know what a single server using this CPU configuration can support. You can add new servers accounting for 17 VMs each as needed without having to re-calculate.</p>
<p>You will also need to take into account the following:</p>
<ul class="simple">
<li>This model assumes you are not oversubscribing your CPU.</li>
<li>If you are considering Hyperthreading, count each core as 1.3, not 2.</li>
<li>Choose a good value CPU that supports the technologies you require.</li>
</ul>
</div>
<div class="section" id="memory">
<h2>Memory<a class="headerlink" href="#memory" title="Permalink to this headline">¶</a></h2>
<p>Continuing to use the example from the previous section, we need to determine how much RAM will be required to support 17 VMs per server. Let&#8217;s assume that you need an average of 4GBs of RAM per VM with dynamic allocation for up to 12GBs for each VM. Calculating that all VMs will be using 12GBs of RAM requires that each server have 204GBs of available RAM.</p>
<p>You must also consider that the node itself needs sufficient RAM to accomodate core OS operations as well as RAM for each VM container (not the RAM allocated to each VM, but the memory the core OS uses to run the VM). The node&#8217;s OS must run it&#8217;s own operations, schedule proccesses, allocate dynamic resources, and handle network operations, so giving the node itself at least 16GBs or more RAM is not unreasonable.</p>
<p>Considering that the RAM we would consider for servers comes in 4GB, 8GB, 16GB, and 32GB sticks, we would need a total of 265GBs of RAM installed per server. For an average 2-CPU server board you get 16-24 RAM slots. To have 256GBs installed you would need sixteen 16GB sticks of RAM to satisfy your RAM needs for up to 17 VMs requiring dynamic allocation up to 12GBs and to support all core OS requirements.</p>
<p>You can adjust this calculation based on your needs.</p>
</div>
<div class="section" id="storage-space">
<h2>Storage Space<a class="headerlink" href="#storage-space" title="Permalink to this headline">¶</a></h2>
<p>When it comes to disk space there are several types that you need to consider:</p>
<ul class="simple">
<li>Ephemeral (the local drive space for a VM)</li>
<li>Persistent (the remote volumes that can be attached to a VM)</li>
<li>Object Storage (such as images or other objects)</li>
</ul>
<p>As far as local drive space that must reside on the compute nodes, in our example of 100 VMs we make the following assumptions:</p>
<ul class="simple">
<li>150 GB local storage per VM</li>
<li>5 TB total of local storage (100 VMs * 50 GB per VM)</li>
<li>500 GB of persistent volume storage per VM</li>
<li>50 TB total persistent storage</li>
</ul>
<p>Returning to our already established example, we need to figure out how much storage to install per server. This storage will service the 17 VMs per server. If we are assuming 50GBs of storage for each VMs drive container, then we would need to install 2.5TBs of storage on the server. Since most servers have anywhere from 4 to 32 2.5&#8221; drive slots or 2 to 12 3.5&#8221; drive slots, depending on server form factor (i.e., 2U vs. 4U), you will need to consider how the storage will be impacted by the intended use.</p>
<p>If storage impact is not expected to be significant, then you may consider using unified storage. For this example a single 3TB drive would provide more than enough storage for 17 150GB VMs. If speed is really not an issue, you might even consider installing two or three 3TB drives and configure a RAID-1 or RAID-5 for redundancy. If speed is critical, however, you will likely want to have a single hardware drive for each VM. In this case you would likely look at a 3U form factor with 24-slots.</p>
<p>Don&#8217;t forget that you will also need drive space for the node itself, and don&#8217;t forget to order the correct backplane that supports the drive configuration that meets your needs. Using our example specifications and assuming that speed it critical, a single server would need 18 drives, most likely 2.5&#8221; 15,000RPM 146GB SAS drives.</p>
<div class="section" id="throughput">
<h3>Throughput<a class="headerlink" href="#throughput" title="Permalink to this headline">¶</a></h3>
<p>As far as throughput, that&#8217;s going to depend on what kind of storage you choose.  In general, you calculate IOPS based on the packing density (drive IOPS * drives in the server / VMs per server), but the actual drive IOPS will depend on the drive technology you choose.  For example:</p>
<ul>
<li><p class="first">3.5&#8221; slow and cheap (100 IOPS per drive, with 2 mirrored drives)</p>
<blockquote>
<div><ul class="simple">
<li>100 IOPS * 2 drives / 17 VMs per server = 12 Read IOPS, 6 Write IOPS</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">2.5&#8221; 15K (200 IOPS, 4 600 GB drive, RAID 10)</p>
<blockquote>
<div><ul class="simple">
<li>200 IOPS * 4 drives / 17 VMs per server = 48 Read IOPS, 24 Write IOPS</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">SSD (40K IOPS, 8 300 GB drive, RAID 10)</p>
<blockquote>
<div><ul class="simple">
<li>40K * 8 drives / 17 VMs per server = 19K Read IOPS, 9.5K Write IOPS</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Clearly, SSD gives you the best performance, but the difference in cost between SSDs and the less costly platter-based solutions is going to be signficant, to say the least. The acceptable cost burden is determined by the balance between your budget and your performance and redundancy needs. It is also important to note that the rules for redundancy in a cloud environment are different than a traditional server installation in that entire servers provide redundancy as opposed to making a single server instance redundant.</p>
<p>In other words, the weight for redundant components shifts from individual OS installation to server redundancy. It is far more critical to have redundant power supplies and hot-swappable CPUs and RAM than to have redundant compute node storage. If, for example, you have 18 drives installed on a server and have 17 drives directly allocated to each VM installed and one fails, you simply replace the drive and push a new node copy. The remaining VMs carry whatever additional load is present due to the temporary loss of one node.</p>
</div>
<div class="section" id="remote-storage">
<h3>Remote storage<a class="headerlink" href="#remote-storage" title="Permalink to this headline">¶</a></h3>
<p>IOPS will also be a factor in determining how you plan to handle persistent storage.  For example, consider these options for laying out your 50 TB of remote volume space:</p>
<ul class="simple">
<li>12 drive storage frame using 3 TB 3.5&#8221; drives mirrored<ul>
<li>36 TB raw, or 18 TB usable space per 2U frame</li>
<li>3 frames (50 TB / 18 TB per server)</li>
<li>12 slots x 100 IOPS per drive = 1200 Read IOPS, 600 Write IOPS per frame</li>
<li>3 frames x 1200 IOPS per frame / 100 VMs = 36 Read IOPS, 18 Write IOPS per VM</li>
</ul>
</li>
<li>24 drive storage frame using 1TB 7200 RPM 2.5&#8221; drives<ul>
<li>24 TB raw, or 12 TB usable space per 2U frame</li>
<li>5 frames (50 TB / 12 TB per server)</li>
<li>24 slots x 100 IOPS per drive = 2400 Read IOPS, 1200 Write IOPS per frame</li>
<li>5 frames x 2400 IOPS per frame / 100 VMs = 120 Read IOPS, 60 Write IOPS per frame</li>
</ul>
</li>
</ul>
<p>You can accomplish the same thing with a single 36 drive frame using 3 TB drives, but this becomes a single point of failure in your cluster.</p>
</div>
<div class="section" id="object-storage">
<h3>Object storage<a class="headerlink" href="#object-storage" title="Permalink to this headline">¶</a></h3>
<p>When it comes to object storage, you will find that you need more space than you think.  For example, this example specifies 50 TB of object storage.  Easy right?  Not really.  Object storage uses a default of 3 times the required space for replication, which means you will need 150 TB.  However, to accommodate two hands-off zones, you will need 5 times the required space, which actually means 250 TB.  The calculations don&#8217;t end there.  You don&#8217;t ever want to run out of space, so &#8220;full&#8221; should really be more like 75% of capacity, which means you will need a total of 333 TB, or a multiplication factor of 6.66.</p>
<p>Of course, that might be a bit much to start with; you might want to start with a happy medium of a multiplier of 4, then acquire more hardware as your drives begin to fill up.  That calculates to 200 TB in our example.  So how do you put that together?  If you were to use 3 TB 3.5&#8221; drives, you could use a 12 drive storage frame, with 6 servers hosting 36 TB each (for a total of 216 TB).  You could also use a 36 drive storage frame, with just 2 servers hosting 108 TB each, but its not recommended due to the high cost of failure to replication and capacity issues.</p>
</div>
</div>
<div class="section" id="networking">
<h2>Networking<a class="headerlink" href="#networking" title="Permalink to this headline">¶</a></h2>
<p>Perhaps the most complex part of designing an OpenStack cluster is the networking.  An OpenStack cluster can involve multiple networks even beyond the Public, Private, and Internal networks.  Your cluster may involve tenant networks, storage networks, multiple tenant private networks, and so on.  Many of these will be VLANs, and all of them will need to be planned out in advance to avoid configuration issues.</p>
<p>In terms of the example network, consider these assumptions:</p>
<ul class="simple">
<li>100 Mbits/second per VM</li>
<li>HA architecture</li>
<li>Network Storage is not latency sensitive</li>
</ul>
<p>In order to achieve this, you can use two 1Gb links per server (2 x 1000 Mbits/second / 17 VMs = 118 Mbits/second).  Using two links also helps with HA.  You can also increase throughput and decrease latency by using 2 10 Gb links, bringing the bandwidth per VM to 1 Gb/second, but if you&#8217;re going to do that, you&#8217;ve got one more factor to consider.</p>
<div class="section" id="scalability-and-oversubscription">
<h3>Scalability and oversubscription<a class="headerlink" href="#scalability-and-oversubscription" title="Permalink to this headline">¶</a></h3>
<p>It is one of the ironies of networking that 1Gb Ethernet generally scales better than 10Gb Ethernet &#8211; at least until 100Gb switches are more commonly available.  It&#8217;s possible to aggregate the 1Gb links in a 48 port switch, so that you have 48 1Gb links down, but 4 10GB links up.  Do the same thing with a 10Gb switch, however, and you have 48 10Gb links down and 4 100Gb links up, resulting in oversubscription.</p>
<p>Like many other issues in OpenStack, you can avoid this problem to a great extent with careful planning.  Problems only arise when you are moving between racks, so plan to create &#8220;pods&#8221;, each of which includes both storage and compute nodes.  Generally, a pod is the size of a non-oversubscribed L2 domain.</p>
</div>
<div class="section" id="hardware-for-this-example">
<h3>Hardware for this example<a class="headerlink" href="#hardware-for-this-example" title="Permalink to this headline">¶</a></h3>
<p>In this example, you are looking at:</p>
<ul class="simple">
<li>2 data switches (for HA), each with a minimum of 12 ports for data (2 x 1Gb links per server x 6 servers)</li>
<li>1 1Gb switch for IPMI (1 port per server x 6 servers)</li>
<li>Optional Cluster Management switch, plus a second for HA</li>
</ul>
<p>Because your network will in all likelihood grow, it&#8217;s best to choose 48 port switches.  Also, as your network grows, you will need to consider uplinks and aggregation switches.</p>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In general, your best bet is to choose a 2 socket server with a balance in I/O, CPU, Memory, and Disk that meets your project requirements.  Look for a 1U R-class or 2U high density C-class server.  Some good options from Dell for compute nodes include:</p>
<ul class="simple">
<li>Dell PowerEdge R620</li>
<li>Dell PowerEdge C6220 Rack Server</li>
<li>Dell PowerEdge R720XD (for high disk or IOPS requirements)</li>
</ul>
<p>You may also want to consider systems from HP (<a class="reference external" href="http://www.hp.com/servers">http://www.hp.com/servers</a>) or from a smaller systems builder like Aberdeen, a manufacturer that specializes in powerful, low-cost systems and storage servers (<a class="reference external" href="http://www.aberdeeninc.com">http://www.aberdeeninc.com</a>).</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li><a href="../../index.html">Fuel for OpenStack 3.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Mirantis.
      Last updated on 2013/07/22.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2b1.
    </div>
  </body>
</html>